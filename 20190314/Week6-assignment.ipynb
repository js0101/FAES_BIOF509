{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e1dc2ec36a962ef67adee71914d4c1bd",
     "grade": false,
     "grade_id": "cell-1",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Week 6 - Classification\n",
    "\n",
    "This week's assignment will give you an opportunity to practice your classification skills with the scikit-learn package and learn to implement your own sklearn-like classifier from scratch.\n",
    "\n",
    "In the first component, you will apply logistic regression to a dataset of wine quality. The idea is to predict wine quality based on its chemical properties. Read more about the wine dataset here - https://archive.ics.uci.edu/ml/datasets/wine+quality.\n",
    "\n",
    "In the second component, you will use multinomial naive Bayes to predict wine qualities.\n",
    "\n",
    "In the third component, you will implement your own version of KNN.\n",
    "\n",
    "Note: Due to a heavy use of randomization techniques the exact performance mertics may not be achieved (sometimes even with the random seed set) therefore the tests in assert statements will  validate your solutions roughly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "6d9e5613e9b2c951f67ffc08d594dbcd",
     "grade": false,
     "grade_id": "cell-2",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "90a2f1eb6473b63e7eb8568dd9c8317b",
     "grade": false,
     "grade_id": "cell-3",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Task 1 (1 point) - Logistic Regression\n",
    "\n",
    "In this task you will use RepeatedStratifiedKfold cross-validation to evaluate performance of Logistic Regression on the wine dataset, predicting wine quality.\n",
    "\n",
    "Wine quality can take values between 3-8. However, for this task you need to convert these qualities to a binary value and your model will predict whether a specific wine is \"high quality\". We will define wine with a quality value of __7__ and above as high-quality.\n",
    "\n",
    "In order to get an estimate of how well your model is doing, you will use RepeatedStratifiedKfold cross-validation. This is an sklearn cross-validation iterator that runs stratified K-fold cross-valdiation multipe times.\n",
    "\n",
    "We will assess how well we are doing using __accuracy__. To do this using the cross_val_score function, set the \"scoring\" parameter to 'accuracy' (scoring='accuracy').\n",
    "\n",
    "Store the wine features in a variable named \"X\", and the response in a variable named \"y\". Store the cross-validation scores in a \"scores\" variable. We will check these variables in our asserts.\n",
    "\n",
    "Additional instructions:\n",
    "* Use the following from sklearn:  RepeatedStratifiedKFold, LogisticRegression and cross_val_score.\n",
    "* We will load the wine dataset for you. You will need to divide the wine dataset into X and y.\n",
    "* X should be a matrix of all the features in the dataset, __excluding wine quality__.\n",
    "* y should be a vector of 0s and 1s, with 0 denoting low-quality wines and 1 denoting high-quality wines.\n",
    "* Initialize the Repeated K-fold cross validation object to have 5 folds and 1000 repetitions. __Set random_state to 42__.\n",
    "* For a visual aid, you can plot the distribution of scores across the cross-validation runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "44e13f98397b310ee4e435c9ea56767c",
     "grade": false,
     "grade_id": "cell-4",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Here we are loading the wine data for you.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n",
    "wines = pd.read_csv(url, sep=\";\")\n",
    "\n",
    "# YOUR CODE HERE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "# Set up data\n",
    "X = wines.drop(columns='quality')\n",
    "y = wines.quality > 7\n",
    "# Initialize\n",
    "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=1000, random_state=42)\n",
    "model = LogisticRegression(n_jobs=-1)\n",
    "# Score\n",
    "scores = cross_val_score(model, X, y, cv=cv, scoring=\"accuracy\", n_jobs=-1)\n",
    "# Plot\n",
    "pd.Series(scores).hist()\n",
    "# plt.show()\n",
    "# plt.close()\n",
    "\n",
    "# To view scores you can run: pd.Series(scores).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "aa657ecaa7603fe433ee77e1419cfc72",
     "grade": true,
     "grade_id": "cell-5",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert y.shape == (1599,)\n",
    "assert X.shape == (1599, 11)\n",
    "assert  pd.Series(scores).mean() > 0.86"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "280c822999661ead532d9f5e925c2891",
     "grade": false,
     "grade_id": "cell-6",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Task 2 (1 point) - Multinomial Naive Bayes\n",
    "\n",
    "In this task you will use RepeatedStratifiedKfold cross-validation to evaluate performance of multinomial naive Bayes (NB) on the wine dataset, predicting wine quality. This is similar to the previous task, except here we are using multinomial NB and we are not converting the wine qualities to binary.\n",
    "\n",
    "For this task, we've decided we want to only use wines with qualities between __5__ and __7__ (inclusive). You will have to exclude wines that are not in this range.\n",
    "\n",
    "In order to get an estimate of how well your model is doing, you will use RepeatedStratifiedKfold cross-validation. This is an sklearn cross-validation iterator that runs stratified K-fold cross-valdiation multipe times.\n",
    "\n",
    "We will assess how well we are doing using __f1_macro__. f1_macro is an evaluation based on the f1 score, but it can be used in multiclass (non-binary) problems. To do this using the cross_val_score function, set the \"scoring\" parameter to 'f1_macro' (scoring='f1_macro').\n",
    "\n",
    "Store the wine features in a variable named \"X\", and the response in a variable named \"y\". Store the cross-validation scores in a \"scores\" variable. We will check these variables in our asserts.\n",
    "\n",
    "Additional instructions:\n",
    "* Use the following from sklearn:  RepeatedStratifiedKFold, MultinomialNB and cross_val_score.\n",
    "* We will load the wine dataset for you. You will need to divide the wine dataset into X and y.\n",
    "* X should be a matrix of all the features in the dataset, __excluding the wine quality feature__ and __excluding wines with qualities that are less than 5 or greater than 7__.\n",
    "* y should be a vector of wine qualities ranging from 5 to 7 (inclusive).\n",
    "* Initialize the Repeated K-fold cross validation object to have 5 splits and 1000 repetitions. __Set random_state to 42__.\n",
    "* For a visual aid, you can plot the distribution of scores across the cross-validation runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "lines_to_next_cell": 2,
    "nbgrader": {
     "checksum": "0dda32d56675976569c504156dbe6baa",
     "grade": false,
     "grade_id": "cell-7",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n",
    "wines = pd.read_csv(url, sep=\";\")\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# modify wines\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "wines = wines[wines.quality >= 5][wines.quality <= 7]\n",
    "X = wines.drop(columns=['quality'])\n",
    "y = wines.quality\n",
    "# Initialize\n",
    "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=1000, random_state=42)\n",
    "model = MultinomialNB()\n",
    "# Score\n",
    "scores = cross_val_score(model, X, y, cv=cv, scoring=\"f1_macro\", n_jobs=-1)\n",
    "# Plot\n",
    "pd.Series(scores).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "lines_to_next_cell": 1,
    "nbgrader": {
     "checksum": "322f2a5d9a03bf59073cb0dda276d528",
     "grade": true,
     "grade_id": "cell-8",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert y.shape == (1518,)\n",
    "assert X.shape == (1518, 11)\n",
    "assert  .4 < pd.Series(scores).mean() < .5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "48951e31581b8d0789a77259de231a67",
     "grade": false,
     "grade_id": "cell-9",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Task 3  (2 points) - KNN\n",
    "\n",
    "Implement your own version of Nearest Neighbor classifier as a class named MyKNN. The only hyperparameter to \\_\\_init\\_\\_(self, K) is K - the number of neighbors.\n",
    "\n",
    "Implement .fit(X, y) method. There is no real training as the model simply memorizes all the data samples and their class labels.\n",
    "\n",
    "Implement .predict(X_new) method; this is where all the calculations are. You need to compare each sample in X_new to the memorized data and choose the K nearest neighbors using euclidean distance. Then you need to predict the label based on the most frequent label of the K neighbors and return an array of predicted labels. The problem could be binary or multiclass, it does not matter for the implementation, however you should predict only one label for each sample.\n",
    "\n",
    "Do not worry about memory or speed optimization, we will not use MyKNN on large datasets.\n",
    "\n",
    "Hints:\n",
    "* Your final class should have at least three functions: \\_\\_init\\_\\_, fit, predict\n",
    "* You will need to implement a function that can calculate the Euclidean distance between two points\n",
    "* The formula for Euclidean distance between two points a and b in python is: np.sum((a - b)\\*\\*2)\\*\\*(1/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "lines_to_next_cell": 1,
    "nbgrader": {
     "checksum": "59b046432967e4773e31b2c65babc0ce",
     "grade": false,
     "grade_id": "t1",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "class MyKNN(object):\n",
    "\n",
    "    \"\"\"Homemade implementation of KNN\"\"\"\n",
    "\n",
    "    def __init__(self, K):\n",
    "        \"\"\"Initialize with hyperparameter K \"\"\"\n",
    "        self.K = K\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit method for training MyKNN\n",
    "\n",
    "        :X: TODO\n",
    "        :y: TODO\n",
    "        :returns: TODO\n",
    "\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def _dist(self, a, b):\n",
    "        return np.sum((a - b)**2)**(1/2)\n",
    "    \n",
    "    def _find_knn(self, a):\n",
    "        \"\"\"Finds knn to point a\n",
    "\n",
    "        :a: point to be queried\n",
    "        :returns: Index of KNN\n",
    "\n",
    "        \"\"\"\n",
    "        dist = np.array([_dist(a, i) for i in self.X])\n",
    "        return np.argpartition(dist, self.K)[:self.K]\n",
    "\n",
    "    def predict(self, X_new):\n",
    "        \"\"\"Predict method for new data\n",
    "\n",
    "        :X_new: TODO\n",
    "        :returns: TODO\n",
    "\n",
    "        \"\"\"\n",
    "        y_new = np.empty(len(X_new))\n",
    "        for i, j  in enumerate(X_new):\n",
    "            idx = _find_knn(j)\n",
    "            labels = self.y[idx]\n",
    "            unique, counts = np.unique(labels, return_counts=True)\n",
    "            labels[i] = unique[np.argmax(counts)]\n",
    "        return y_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "067fbc1556e4e698bd49ef6949ee7ed2",
     "grade": true,
     "grade_id": "t1-test",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "##### Iris dataset ##############################################\n",
    "# 150 samples, 4 features\n",
    "# 3 classes, 50 samples per class\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "X_iris, y_iris = load_iris(return_X_y=True)\n",
    "X_iris_train, X_iris_test, y_iris_train, y_iris_test = train_test_split(X_iris, y_iris, test_size=0.3, stratify=y_iris)\n",
    "cls_iris = MyKNN(5)\n",
    "cls_iris.fit(X_iris_train, y_iris_train)\n",
    "\n",
    "ref_cls_iris = KNeighborsClassifier(5)\n",
    "ref_cls_iris.fit(X_iris_train, y_iris_train)\n",
    "assert accuracy_score(y_iris_test, ref_cls_iris.predict(X_iris_test)) >= 0.9\n",
    "assert accuracy_score(y_iris_test, cls_iris.predict(X_iris_test)) >= 0.9"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
